"""
ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œ ì„ë² ë”© ìŠ¤í¬ë¦½íŠ¸
- .md íŒŒì¼ ì½ê¸°
- Voyage AIë¡œ ì„ë² ë”© ìƒì„±
- Qdrantì— ì €ì¥
"""

import os
import sys
import hashlib
import logging
from pathlib import Path
from datetime import datetime
from tqdm import tqdm

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from dotenv import load_dotenv
load_dotenv(PROJECT_ROOT / ".env")

# í…ìŠ¤íŠ¸ ë¶„í• 
from langchain_text_splitters import RecursiveCharacterTextSplitter

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ
from src.services.embeddings.manager import EmbeddingManager
from src.infrastructure.database.qdrant_manager import QdrantManager

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(PROJECT_ROOT / "logs" / "md_embedding.log", encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)


class MarkdownEmbedder:
    """ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œ ì„ë² ë”© ì²˜ë¦¬ê¸°"""

    def __init__(
        self,
        collection_name: str = "labor_standards_act_commentary",
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
        reset_collection: bool = False
    ):
        """
        ì´ˆê¸°í™”

        Args:
            collection_name: Qdrant ì»¬ë ‰ì…˜ ì´ë¦„
            chunk_size: ì²­í¬ í¬ê¸° (ë¬¸ì ìˆ˜)
            chunk_overlap: ì²­í¬ ì˜¤ë²„ë© (ë¬¸ì ìˆ˜)
            reset_collection: Trueë©´ ì»¬ë ‰ì…˜ ì´ˆê¸°í™” (ì‚­ì œ í›„ ì¬ìƒì„±)
        """
        self.collection_name = collection_name
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

        # í…ìŠ¤íŠ¸ ë¶„í• ê¸°
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]
        )

        # ì„ë² ë”© ë§¤ë‹ˆì €
        logger.info("ì„ë² ë”© ë§¤ë‹ˆì € ì´ˆê¸°í™” ì¤‘...")
        self.embedding_manager = EmbeddingManager()

        # Qdrant ë§¤ë‹ˆì €
        logger.info("Qdrant ë§¤ë‹ˆì € ì´ˆê¸°í™” ì¤‘...")
        self.qdrant_manager = QdrantManager()

        # ì»¬ë ‰ì…˜ ì´ˆê¸°í™” (ì„ íƒì )
        if reset_collection:
            self._reset_collection()
        else:
            self._ensure_collection()

        logger.info(f"âœ… MarkdownEmbedder ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"   ì»¬ë ‰ì…˜: {self.collection_name}")
        logger.info(f"   ì²­í¬ í¬ê¸°: {chunk_size}")
        logger.info(f"   ì˜¤ë²„ë©: {chunk_overlap}")

    def _reset_collection(self):
        """ì»¬ë ‰ì…˜ ì´ˆê¸°í™” (ì‚­ì œ í›„ ì¬ìƒì„±)"""
        if self.qdrant_manager.collection_exists(self.collection_name):
            logger.info(f"ğŸ—‘ï¸ ì»¬ë ‰ì…˜ '{self.collection_name}' ì‚­ì œ ì¤‘...")
            self.qdrant_manager.delete_collection(self.collection_name)
            logger.info(f"âœ… ì»¬ë ‰ì…˜ '{self.collection_name}' ì‚­ì œ ì™„ë£Œ")

        # ìƒˆë¡œ ìƒì„±
        self.qdrant_manager.create_collection(
            collection_name=self.collection_name,
            vector_size=self.embedding_manager.dimension
        )
        logger.info(f"âœ… ì»¬ë ‰ì…˜ '{self.collection_name}' ìƒì„± ì™„ë£Œ")

    def _ensure_collection(self):
        """ì»¬ë ‰ì…˜ ì¡´ì¬ í™•ì¸ ë° ìƒì„±"""
        if not self.qdrant_manager.collection_exists(self.collection_name):
            self.qdrant_manager.create_collection(
                collection_name=self.collection_name,
                vector_size=self.embedding_manager.dimension
            )
            logger.info(f"âœ… ì»¬ë ‰ì…˜ '{self.collection_name}' ìƒì„±ë¨")
        else:
            logger.info(f"â„¹ï¸ ì»¬ë ‰ì…˜ '{self.collection_name}' ì´ë¯¸ ì¡´ì¬")

    def read_markdown(self, md_path: Path) -> tuple[str, dict]:
        """
        ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì½ê¸°

        Returns:
            tuple: (í…ìŠ¤íŠ¸ ë‚´ìš©, ë©”íƒ€ë°ì´í„°)
        """
        md_path = Path(md_path)
        metadata = {
            "file_name": md_path.name,
            "file_path": str(md_path),
        }

        try:
            with open(md_path, 'r', encoding='utf-8') as f:
                text = f.read()

            metadata["char_count"] = len(text)
            metadata["line_count"] = text.count('\n') + 1

            return text, metadata

        except Exception as e:
            logger.error(f"ë§ˆí¬ë‹¤ìš´ ì½ê¸° ì‹¤íŒ¨ ({md_path.name}): {e}")
            raise

    def generate_doc_id(self, file_path: str, chunk_index: int) -> str:
        """ë¬¸ì„œ ì²­í¬ì˜ ê³ ìœ  ID ìƒì„±"""
        content = f"{file_path}:{chunk_index}"
        return hashlib.md5(content.encode()).hexdigest()

    def process_markdown(self, md_path: Path) -> int:
        """
        ë‹¨ì¼ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì²˜ë¦¬ ë° ì„ë² ë”©

        Returns:
            int: ì €ì¥ëœ ì²­í¬ ìˆ˜
        """
        md_path = Path(md_path)
        logger.info(f"ğŸ“„ ì²˜ë¦¬ ì¤‘: {md_path.name}")

        # í…ìŠ¤íŠ¸ ì½ê¸°
        text, metadata = self.read_markdown(md_path)

        if not text.strip():
            logger.warning(f"âš ï¸ í…ìŠ¤íŠ¸ ì—†ìŒ: {md_path.name}")
            return 0

        logger.info(f"   í…ìŠ¤íŠ¸ í¬ê¸°: {len(text):,}ì ({metadata['line_count']}ì¤„)")

        # í…ìŠ¤íŠ¸ ë¶„í• 
        chunks = self.text_splitter.split_text(text)
        logger.info(f"   ìƒì„±ëœ ì²­í¬: {len(chunks)}ê°œ")

        if not chunks:
            return 0

        # ì„ë² ë”© ìƒì„± (ë°°ì¹˜)
        logger.info(f"   ì„ë² ë”© ìƒì„± ì¤‘...")
        embeddings = self.embedding_manager.create_embeddings_batch(
            texts=chunks,
            input_type="document"
        )

        # í¬ì¸íŠ¸ ìƒì„±
        points = []
        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
            point_id = self.generate_doc_id(str(md_path), i)

            payload = {
                "text": chunk,
                "source": md_path.name,
                "file_path": str(md_path),
                "chunk_index": i,
                "total_chunks": len(chunks),
                "char_count": metadata["char_count"],
                "category": "ê·¼ë¡œê¸°ì¤€ë²•ì£¼í•´",
                "document_type": "legal_commentary",
                "created_at": datetime.now().isoformat()
            }

            points.append({
                "id": point_id,
                "vector": embedding,
                "payload": payload
            })

        # Qdrantì— ì—…ë¡œë“œ
        success = self.qdrant_manager.upsert_points(
            points=points,
            collection_name=self.collection_name
        )

        if success:
            logger.info(f"   âœ… {len(points)}ê°œ ì²­í¬ ì €ì¥ ì™„ë£Œ")
            return len(points)
        else:
            logger.error(f"   âŒ ì €ì¥ ì‹¤íŒ¨")
            return 0

    def process_directory(self, dir_path: str) -> dict:
        """
        ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì²˜ë¦¬

        Returns:
            dict: ì²˜ë¦¬ ê²°ê³¼ í†µê³„
        """
        dir_path = Path(dir_path)

        if not dir_path.exists():
            raise FileNotFoundError(f"ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {dir_path}")

        # ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì°¾ê¸°
        md_files = sorted(list(dir_path.glob("**/*.md")))
        logger.info(f"ğŸ” ë°œê²¬ëœ ë§ˆí¬ë‹¤ìš´ íŒŒì¼: {len(md_files)}ê°œ")

        if not md_files:
            logger.warning("ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
            return {"total": 0, "success": 0, "failed": 0, "chunks": 0}

        stats = {
            "total": len(md_files),
            "success": 0,
            "failed": 0,
            "chunks": 0,
            "failed_files": []
        }

        # ì§„í–‰ í‘œì‹œì™€ í•¨ê»˜ ì²˜ë¦¬
        for md_path in tqdm(md_files, desc="ë§ˆí¬ë‹¤ìš´ ì²˜ë¦¬"):
            try:
                chunks = self.process_markdown(md_path)
                if chunks > 0:
                    stats["success"] += 1
                    stats["chunks"] += chunks
                else:
                    stats["failed"] += 1
                    stats["failed_files"].append(str(md_path))
            except Exception as e:
                logger.error(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨ ({md_path.name}): {e}")
                stats["failed"] += 1
                stats["failed_files"].append(str(md_path))

        return stats


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(description="ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œ ì„ë² ë”©")
    parser.add_argument(
        "--path",
        type=str,
        required=True,
        help="ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” ë””ë ‰í† ë¦¬"
    )
    parser.add_argument(
        "--collection",
        type=str,
        default="labor_standards_act_commentary",
        help="Qdrant ì»¬ë ‰ì…˜ ì´ë¦„"
    )
    parser.add_argument(
        "--chunk-size",
        type=int,
        default=1000,
        help="ì²­í¬ í¬ê¸° (ë¬¸ì ìˆ˜)"
    )
    parser.add_argument(
        "--chunk-overlap",
        type=int,
        default=200,
        help="ì²­í¬ ì˜¤ë²„ë© (ë¬¸ì ìˆ˜)"
    )
    parser.add_argument(
        "--reset",
        action="store_true",
        help="ì»¬ë ‰ì…˜ ì´ˆê¸°í™” (ì‚­ì œ í›„ ì¬ìƒì„±)"
    )

    args = parser.parse_args()

    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    log_dir = PROJECT_ROOT / "logs"
    log_dir.mkdir(exist_ok=True)

    print("=" * 60)
    print("ğŸš€ ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œ ì„ë² ë”© ì‹œì‘")
    print("=" * 60)
    print(f"ğŸ“‚ ê²½ë¡œ: {args.path}")
    print(f"ğŸ“¦ ì»¬ë ‰ì…˜: {args.collection}")
    print(f"ğŸ“ ì²­í¬ í¬ê¸°: {args.chunk_size}")
    print(f"ğŸ”— ì˜¤ë²„ë©: {args.chunk_overlap}")
    print(f"ğŸ—‘ï¸ ì»¬ë ‰ì…˜ ì´ˆê¸°í™”: {args.reset}")
    print("=" * 60)

    try:
        # MarkdownEmbedder ì´ˆê¸°í™”
        embedder = MarkdownEmbedder(
            collection_name=args.collection,
            chunk_size=args.chunk_size,
            chunk_overlap=args.chunk_overlap,
            reset_collection=args.reset
        )

        path = Path(args.path)

        if path.is_file():
            # ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬
            chunks = embedder.process_markdown(path)
            print(f"\nâœ… ì™„ë£Œ: {chunks}ê°œ ì²­í¬ ì €ì¥")
        else:
            # ë””ë ‰í† ë¦¬ ì²˜ë¦¬
            stats = embedder.process_directory(path)

            print("\n" + "=" * 60)
            print("ğŸ“Š ì²˜ë¦¬ ê²°ê³¼")
            print("=" * 60)
            print(f"   ì „ì²´ íŒŒì¼: {stats['total']}ê°œ")
            print(f"   ì„±ê³µ: {stats['success']}ê°œ")
            print(f"   ì‹¤íŒ¨: {stats['failed']}ê°œ")
            print(f"   ì´ ì²­í¬: {stats['chunks']}ê°œ")

            if stats['failed_files']:
                print("\nâŒ ì‹¤íŒ¨í•œ íŒŒì¼:")
                for f in stats['failed_files'][:10]:
                    print(f"   - {f}")
                if len(stats['failed_files']) > 10:
                    print(f"   ... ì™¸ {len(stats['failed_files']) - 10}ê°œ")

        # ì»¬ë ‰ì…˜ ì •ë³´ ì¶œë ¥
        info = embedder.qdrant_manager.get_collection_info(args.collection)
        if info:
            print(f"\nğŸ“¦ ì»¬ë ‰ì…˜ '{args.collection}' í˜„ì¬ ìƒíƒœ:")
            print(f"   ì´ í¬ì¸íŠ¸: {info['points_count']:,}ê°œ")
            print(f"   ìƒíƒœ: {info['status']}")

    except Exception as e:
        logger.error(f"ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        raise


if __name__ == "__main__":
    main()

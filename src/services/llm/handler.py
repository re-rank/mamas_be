"""LLM Handler - LLM ê¸°ë°˜ ë‹µë³€ ìƒì„±"""

import logging
from typing import Any, Optional

from openai import OpenAI

from src.config import app_config as config

logger = logging.getLogger(__name__)


# ê¸°ë³¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
DEFAULT_SYSTEM_PROMPT = """You are a friendly and professional AI assistant.
You provide accurate and useful answers based on the provided context information.

Follow these guidelines when answering:
1. Prioritize information from the provided context
2. If information is not in the context, clearly state "This information is not available in the provided context"
3. Provide clear and structured answers
4. Mention relevant sources when necessary
5. **IMPORTANT: Always respond in the same language as the user's question**
   - If the user asks in Korean, respond in Korean
   - If the user asks in English, respond in English
   - If the user asks in Japanese, respond in Japanese
   - Match the language of any other language the user uses
"""


class LLMHandler:
    """LLM ê¸°ë°˜ ë‹µë³€ ìƒì„± í•¸ë“¤ëŸ¬"""
    
    def __init__(
        self,
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        system_prompt: Optional[str] = None
    ):
        """LLM í•¸ë“¤ëŸ¬ ì´ˆê¸°í™”"""
        self.model = model or config.LLM_MODEL
        self.temperature = temperature if temperature is not None else config.LLM_TEMPERATURE
        self.max_tokens = max_tokens or config.LLM_MAX_TOKENS
        self.system_prompt = system_prompt or DEFAULT_SYSTEM_PROMPT
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = OpenAI(
            api_key=config.OPENAI_API_KEY,
            timeout=config.LLM_TIMEOUT
        )
        
        logger.info("ğŸ¤– LLM í•¸ë“¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"    ëª¨ë¸: {self.model}")
        logger.info(f"    ì˜¨ë„: {self.temperature}")
        logger.info(f"    ìµœëŒ€ í† í°: {self.max_tokens}")
    
    def _build_context(self, search_results: list[dict[str, Any]]) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ë¡œë¶€í„° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        if not search_results:
            return "No relevant information found."

        context_parts = []
        for i, result in enumerate(search_results, 1):
            content = result.get("content", "")
            title = result.get("title", "")
            score = result.get("score", 0)

            if content:
                part = f"[Document {i}]"
                if title:
                    part += f" ({title})"
                part += f" [Relevance: {score:.2f}]\n{content}"
                context_parts.append(part)

        return "\n\n---\n\n".join(context_parts)
    
    def _build_messages(
        self,
        question: str,
        context: str,
        conversation_history: Optional[list[dict]] = None
    ) -> list[dict[str, str]]:
        """LLM ë©”ì‹œì§€ êµ¬ì„±"""
        messages = [
            {"role": "system", "content": self.system_prompt}
        ]
        
        # ëŒ€í™” ê¸°ë¡ ì¶”ê°€
        if conversation_history:
            for msg in conversation_history[-6:]:  # ìµœê·¼ 6ê°œ ë©”ì‹œì§€ë§Œ
                messages.append({
                    "role": msg.get("role", "user"),
                    "content": msg.get("content", "")
                })
        
        # ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ì»¨í…ìŠ¤íŠ¸
        user_message = f"""Please answer the question based on the following context information.

### Context:
{context}

### Question:
{question}

### Answer:"""
        
        messages.append({"role": "user", "content": user_message})
        
        return messages
    
    def generate_answer(
        self,
        question: str,
        search_results: list[dict[str, Any]],
        conversation_history: Optional[list[dict]] = None,
        temperature: Optional[float] = None
    ) -> dict[str, Any]:
        """RAG ê¸°ë°˜ ë‹µë³€ ìƒì„±"""
        try:
            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context = self._build_context(search_results)
            
            # ë©”ì‹œì§€ êµ¬ì„±
            messages = self._build_messages(question, context, conversation_history)
            
            # LLM í˜¸ì¶œ
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature if temperature is not None else self.temperature,
                max_tokens=self.max_tokens
            )
            
            answer = response.choices[0].message.content
            
            # í† í° ì‚¬ìš©ëŸ‰
            usage = {
                "prompt_tokens": response.usage.prompt_tokens if response.usage else 0,
                "completion_tokens": response.usage.completion_tokens if response.usage else 0,
                "total_tokens": response.usage.total_tokens if response.usage else 0
            }
            
            logger.info(f"âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ (í† í°: {usage['total_tokens']})")
            
            return {
                "answer": answer,
                "model": self.model,
                "usage": usage,
                "context_count": len(search_results),
                "success": True
            }
            
        except Exception as e:
            logger.error(f"âŒ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                "answer": "Sorry, an error occurred while generating the answer.",
                "error": str(e),
                "success": False
            }
    
    def generate_answer_stream(
        self,
        question: str,
        search_results: list[dict[str, Any]],
        conversation_history: Optional[list[dict]] = None,
        temperature: Optional[float] = None
    ):
        """ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„±"""
        try:
            context = self._build_context(search_results)
            messages = self._build_messages(question, context, conversation_history)
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature if temperature is not None else self.temperature,
                max_tokens=self.max_tokens,
                stream=True
            )
            
            for chunk in response:
                if chunk.choices and chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
                    
        except Exception as e:
            logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ì‹¤íŒ¨: {e}")
            yield f"An error occurred: {str(e)}"
    
    def chat(
        self,
        message: str,
        conversation_history: Optional[list[dict]] = None
    ) -> str:
        """ì¼ë°˜ ì±„íŒ… (RAG ì—†ìŒ)"""
        try:
            messages = [
                {"role": "system", "content": self.system_prompt}
            ]
            
            if conversation_history:
                for msg in conversation_history[-10:]:
                    messages.append({
                        "role": msg.get("role", "user"),
                        "content": msg.get("content", "")
                    })
            
            messages.append({"role": "user", "content": message})
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"ì±„íŒ… ì‹¤íŒ¨: {e}")
            return f"An error occurred: {str(e)}"
    
    def update_system_prompt(self, prompt: str):
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì—…ë°ì´íŠ¸"""
        self.system_prompt = prompt
        logger.info("ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì—…ë°ì´íŠ¸ë¨")

